{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e231ec5d-4b3a-43a8-a8db-86f09f08cdf8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Path to your extracted file\n",
    "file_path = \"/Volumes/workspace/default/miningaccidents/Accidents.txt\"  # Replace with your local or cloud path\n",
    "\n",
    "# Load the file using pipe delimiter and quoted values\n",
    "df = pd.read_csv(\n",
    "    file_path,\n",
    "    sep=\"|\",\n",
    "    quotechar='\"',\n",
    "    encoding=\"latin1\",  # Use 'latin1' encoding to handle decoding errors\n",
    "    dtype=str  # Start with all fields as strings\n",
    ")\n",
    "\n",
    "# Strip whitespace from column names and convert to lowercase\n",
    "df.columns = [col.strip().lower().replace(\" \", \"_\") for col in df.columns]\n",
    "\n",
    "# Convert relevant date fields\n",
    "df['accident_dt'] = pd.to_datetime(df['accident_dt'], errors='coerce')\n",
    "df['return_to_work_dt'] = pd.to_datetime(df['return_to_work_dt'], errors='coerce')\n",
    "\n",
    "# Drop rows missing crucial information (e.g., mine_id or accident_dt)\n",
    "df = df.dropna(subset=[\"mine_id\", \"accident_dt\"])\n",
    "\n",
    "# Preview cleaned data\n",
    "# print(df.head())\n",
    "\n",
    "# Convert Pandas DataFrame to Spark DataFrame\n",
    "spark_df = spark.createDataFrame(df)\n",
    "\n",
    "# Save as Delta table in the default workspace\n",
    "table_name = \"mine_accidents\"\n",
    "spark_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(table_name)\n",
    "\n",
    "print(f\"ðŸŽ‰ Data saved as Delta table: {table_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34699538-c8ea-48e8-8e88-9011e7f693e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Analyze the most common types of accidents\n",
    "common_accidents = df['accident_type'].value_counts().head(10)\n",
    "\n",
    "display(common_accidents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "837ca8fd-82c5-4cf5-b314-e86f36ccdf41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Not Required\n",
    "# # Gettng address data\n",
    "# #/Volumes/workspace/default/miningaccidents/AddressOfRecord.txt\n",
    "\n",
    "# import pandas as pd\n",
    "\n",
    "# # Path to your extracted file\n",
    "# file_path_add = \"/Volumes/workspace/default/miningaccidents/AddressOfRecord.txt\"  # Replace with your local or cloud path\n",
    "\n",
    "# # Load the file using pipe delimiter and quoted values\n",
    "# df_add = pd.read_csv(\n",
    "#     file_path_add,\n",
    "#     sep=\"|\",\n",
    "#     quotechar='\"',\n",
    "#     encoding=\"latin1\",  # Use 'latin1' encoding to handle decoding errors\n",
    "#     dtype=str  # Start with all fields as strings\n",
    "# )\n",
    "\n",
    "# # Strip whitespace from column names and convert to lowercase\n",
    "# df_add.columns = [col.strip().lower().replace(\" \", \"_\") for col in df_add.columns]\n",
    "\n",
    "# # print(df_add.head())\n",
    "# df_add.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90049777-6aaf-41ae-aa39-26ab13b0620d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#not required\n",
    "# df_add = df_add.fillna('')\n",
    "# # Combine zip_cd and state into one column\n",
    "# df_add['zip_state'] = df_add.apply(lambda row: f\"{row['zip_cd']}, {row['state']}\", axis=1)\n",
    "# # Replace empty cells with blanks\n",
    "# display(df_add['zip_state'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b3f8b69-2f3c-42ca-8b43-56da1a1b804b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Getting mines data location co-ordinates\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Path to your extracted file\n",
    "file_path_add = \"/Volumes/workspace/default/miningaccidents/Mines.txt\" # Replace with your local or cloud path\n",
    "\n",
    "# Load the file using pipe delimiter and quoted values\n",
    "df_add = pd.read_csv(\n",
    "    file_path_add,\n",
    "    sep=\"|\",\n",
    "    quotechar='\"',\n",
    "    encoding=\"latin1\",  # Use 'latin1' encoding to handle decoding errors\n",
    "    dtype=str  # Start with all fields as strings\n",
    ")\n",
    "\n",
    "# Strip whitespace from column names and convert to lowercase\n",
    "df_add.columns = [col.strip().lower().replace(\" \", \"_\") for col in df_add.columns]\n",
    "\n",
    "# print(df_add.head())\n",
    "df_add.display()\n",
    "\n",
    "# Convert Pandas DataFrame to Spark DataFrame\n",
    "spark_df_mine_locations = spark.createDataFrame(df_add)\n",
    "\n",
    "# Save as Delta table in the default workspace\n",
    "table_name = \"mine_locations\"\n",
    "spark_df_mine_locations.write.format(\"delta\").mode(\"overwrite\").saveAsTable(table_name)\n",
    "\n",
    "print(f\"ðŸŽ‰ Data saved as Delta table: {table_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7dd67224-17c4-4cb0-8ca9-d029dff7fb00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "from pyspark.sql.functions import to_date\n",
    "\n",
    "# Load the mine_accidents table\n",
    "mine_accidents_df = spark.table(\"default.mine_accidents\")\n",
    "\n",
    "# Load the mine_locations table\n",
    "mine_locations_df = spark.table(\"default.mine_locations\")\n",
    "\n",
    "# Rename the 'coal_metal_ind' column in mine_locations_df to avoid conflict\n",
    "mine_locations_df = mine_locations_df.withColumnRenamed(\"coal_metal_ind\", \"coal_metal_ind_loc\")\n",
    "\n",
    "# Rename the 'mine_id' column in mine_locations_df to avoid conflict\n",
    "mine_locations_df = mine_locations_df.withColumnRenamed(\"mine_id\", \"mine_id_loc\")\n",
    "\n",
    "# Join the two DataFrames on the common key 'mine_id' and 'current_controller_id'\n",
    "combined_df = mine_accidents_df.join(\n",
    "    mine_locations_df,\n",
    "    on=[mine_accidents_df.mine_id == mine_locations_df.mine_id_loc, \n",
    "        mine_accidents_df.controller_id == mine_locations_df.current_controller_id],\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "# Convert 'accident_dt' to proper date format\n",
    "combined_df = combined_df.withColumn(\"accident_dt\", to_date(combined_df[\"accident_dt\"]))\n",
    "\n",
    "# Drop the existing Delta table if it exists\n",
    "spark.sql(\"DROP TABLE IF EXISTS default.combined_mine_data\")\n",
    "\n",
    "# Save the combined DataFrame as a Delta table in the default schema\n",
    "combined_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"default.combined_mine_data\")\n",
    "\n",
    "print(\"ðŸŽ‰ Data saved as Delta table: default.combined_mine_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47322911-58b2-41e9-8384-720df444a619",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1752076631278}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load the combined_mine_data table\n",
    "combined_mine_data_df = spark.table(\"default.combined_mine_data\")\n",
    "\n",
    "# Example query: Select specific columns and filter by a condition\n",
    "query = \"\"\"\n",
    "SELECT distinct latitude, longitude,current_mine_name\n",
    "FROM default.combined_mine_data\n",
    "where fiscal_yr > 2020\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query\n",
    "result_df = spark.sql(query)\n",
    "\n",
    "# Display the result\n",
    "display(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b76be81-cc62-4dad-b742-f1eb3672b955",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(result_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05ed678e-f663-41ed-b5a1-70b96136f86c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Assuming df is already defined and loaded\n",
    "# Join df to df_add on a common key, e.g., 'mine_id'\n",
    "df_combined = pd.merge(df, df_add, on='mine_id', how='inner')\n",
    "\n",
    "# Display the combined DataFrame\n",
    "display(df_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f75a4468-6935-4315-b0f8-83716ced2734",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# not required\n",
    "\n",
    "# import pandas as pd\n",
    "# from geopy.geocoders import Nominatim\n",
    "# from geopy.exc import GeocoderTimedOut, GeocoderServiceError\n",
    "# import time\n",
    "\n",
    "# # Example DataFrame (remove this if you're using your actual df_add)\n",
    "# # df_add = pd.DataFrame({'zip_cd': ['35040', '10001'], 'state': ['Alabama', 'New York']})\n",
    "\n",
    "# # Step 1: Combine ZIP code and state into one column\n",
    "# df_add['zip_state'] = df_add.apply(lambda row: f\"{row['zip_cd']}, {row['state']}\", axis=1)\n",
    "\n",
    "# # Step 2: Initialize geolocator\n",
    "# geolocator = Nominatim(user_agent=\"geoapi\")\n",
    "\n",
    "# # Step 3: Define geocoding function with retry and caching\n",
    "# geo_cache = {}\n",
    "\n",
    "# def get_lat_long(location):\n",
    "#     if location in geo_cache:\n",
    "#         return geo_cache[location]\n",
    "    \n",
    "#     try:\n",
    "#         loc = geolocator.geocode(location)\n",
    "#         time.sleep(1)  # Respect Nominatim rate limits\n",
    "\n",
    "#         if loc:\n",
    "#             result = (loc.latitude, loc.longitude)\n",
    "#         else:\n",
    "#             result = (None, None)\n",
    "        \n",
    "#         geo_cache[location] = result\n",
    "#         return result\n",
    "    \n",
    "#     except (GeocoderTimedOut, GeocoderServiceError):\n",
    "#         time.sleep(2)\n",
    "#         return get_lat_long(location)\n",
    "\n",
    "# # Step 4: Apply the function and split into new columns\n",
    "# df_add[['latitude', 'longitude']] = df_add['zip_state'].apply(get_lat_long).apply(pd.Series)\n",
    "\n",
    "# # Step 5: Preview the DataFrame\n",
    "# display(df_add.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca0c8bba-5392-4cde-ac40-c5f811d23982",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# not required\n",
    "\n",
    "# %pip install opencage\n",
    "\n",
    "# import pandas as pd\n",
    "# from opencage.geocoder import OpenCageGeocode\n",
    "# import time\n",
    "\n",
    "# # Your OpenCage API Key\n",
    "# key = '1980097060d7409a8811e0a65f5b60b1'\n",
    "# geocoder = OpenCageGeocode(key)\n",
    "\n",
    "# # Combine ZIP and state\n",
    "# df_add['zip_state'] = df_add.apply(lambda row: f\"{row['zip_cd']}, {row['state']}\", axis=1)\n",
    "\n",
    "# # Caching to avoid repeat lookups\n",
    "# geo_cache = {}\n",
    "\n",
    "# def get_lat_long(location):\n",
    "#     if location in geo_cache:\n",
    "#         return geo_cache[location]\n",
    "    \n",
    "#     try:\n",
    "#         result = geocoder.geocode(location)\n",
    "#         time.sleep(0.2)  # avoid hitting rate limits\n",
    "\n",
    "#         if result and len(result):\n",
    "#             lat = result[0]['geometry']['lat']\n",
    "#             lng = result[0]['geometry']['lng']\n",
    "#             geo_cache[location] = (lat, lng)\n",
    "#             return lat, lng\n",
    "#         else:\n",
    "#             geo_cache[location] = (None, None)\n",
    "#             return None, None\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error for {location}: {e}\")\n",
    "#         return None, None\n",
    "\n",
    "# # Apply to DataFrame\n",
    "# df_add[['latitude', 'longitude']] = df_add['zip_state'].apply(get_lat_long).apply(pd.Series)\n",
    "\n",
    "# # Preview\n",
    "# df_add.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8c1ef5f-10b7-457c-928f-d67e2c04cfce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # %python\n",
    "# # %pip install tabulate\n",
    "# import tabulate\n",
    "# import json\n",
    "# import pandas as pd\n",
    "\n",
    "# # Path to the downloaded file\n",
    "# file_path = '/Volumes/workspace/default/miningaccidents/lite.json'  # Update this if needed\n",
    "\n",
    "# # Step 1: Open and load JSON from the file\n",
    "# with open(file_path, 'r', encoding='utf-8') as f:\n",
    "#     stations = json.load(f)\n",
    "\n",
    "# # # Load the data again in case the previous state is not preserved\n",
    "# df = pd.read_json(file_path)\n",
    "\n",
    "# # Flatten the 'name' column\n",
    "# df_name = pd.json_normalize(df['name'], sep='_')\n",
    "# df = pd.concat([df, df_name], axis=1)\n",
    "\n",
    "# # Flatten the 'identifiers' column\n",
    "# df_identifiers = pd.json_normalize(df['identifiers'], sep='_')\n",
    "# df = pd.concat([df, df_identifiers], axis=1)\n",
    "\n",
    "# # Flatten the 'location' column\n",
    "# df_location = pd.json_normalize(df['location'], sep='_')\n",
    "# df = pd.concat([df, df_location], axis=1)\n",
    "\n",
    "# # Flatten the 'inventory' column\n",
    "# df_inventory = pd.json_normalize(df['inventory'], sep='_')\n",
    "# df = pd.concat([df, df_inventory], axis=1)\n",
    "\n",
    "# # Drop the original nested columns\n",
    "# df = df.drop(columns=['name', 'identifiers', 'location', 'inventory'])\n",
    "\n",
    "# # Display the first 5 rows of the flattened DataFrame\n",
    "# # print(df.head().to_markdown(index=False, numalign=\"left\", stralign=\"left\"))\n",
    "\n",
    "# # Print the column names and their data types\n",
    "# #print(df.info())\n",
    "# df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d81cd22-b8fd-4eb3-b458-135f0358ccd7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from geopy.distance import geodesic\n",
    "\n",
    "def find_nearest_station(mine_coord, df):\n",
    "        df['distance_km'] = df.apply(\n",
    "        lambda row: geodesic(mine_coord, (row['latitude'], row['longitude'])).km, axis=1\n",
    "    )\n",
    "        return df.sort_values('distance_km').iloc[0]\n",
    "\n",
    "# Example\n",
    "mine_location = ( 41.305,-94.453333)\n",
    "nearest_station = find_nearest_station(mine_location, df)\n",
    "print(nearest_station)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f33be40-924f-4b8c-907b-a28115dd628f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %pip install pandas==2.0.3\n",
    "%restart_python\n",
    "# Update the meteostat library\n",
    "# %pip install --upgrade meteostat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ab000ca-31e2-4e46-bfbb-7518424776d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "# Update the meteostat library\n",
    "# %pip install --upgrade meteostat\n",
    "\n",
    "from meteostat import Point, Daily\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "# Define the station and date range\n",
    "station = Point( 41.305,-94.453333)  # Example: Denver\n",
    "start = datetime(2021, 2, 24) --2008-05-13\n",
    "end = datetime(2021, 2, 24)\n",
    "\n",
    "# Fetch daily weather data\n",
    "data = Daily(station, start, end)\n",
    "weather_df = data.fetch()\n",
    "\n",
    "# OPTIONAL: Replace missing values with None (if needed)\n",
    "weather_df = weather_df.where(pd.notnull(weather_df), None)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(weather_df)\n",
    "\n",
    "# 44.146667\t-72.481111\n",
    "# 2021-02-24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cde83ed7-6576-4b42-9e2e-41f1d9766da3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from meteostat import Point, Daily\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Example accident data\n",
    "accidents = pd.DataFrame({\n",
    "    'accident_id': [1, 2],\n",
    "    'date': ['2023-07-15', '2023-11-02'],\n",
    "    'latitude': [39.7392, 40.01499],\n",
    "    'longitude': [-104.9903, -105.2705]\n",
    "    # 44.14, 72.48\n",
    "})\n",
    "\n",
    "# Function to get weather\n",
    "def get_weather(lat, lon, date):\n",
    "    location = Point(lat, lon)\n",
    "    start = datetime.strptime(date, \"%Y-%m-%d\")\n",
    "    end = start  # Fetch data only for the accident day\n",
    "\n",
    "    try:\n",
    "        data = Daily(location, start, end)\n",
    "        data = data.fetch()\n",
    "\n",
    "        if not data.empty:\n",
    "            # Return temperature, precipitation, etc.\n",
    "            return {\n",
    "                'temp_c': data['tavg'].iloc[0],\n",
    "                'precip_mm': data['prcp'].iloc[0],\n",
    "                'snow_cm': data['snow'].iloc[0],\n",
    "                'wind_kmh': data['wspd'].iloc[0]\n",
    "            }\n",
    "        else:\n",
    "            return {'temp_c': None, 'precip_mm': None, 'snow_cm': None, 'wind_kmh': None}\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching weather for {lat}, {lon} on {date}: {e}\")\n",
    "        return {'temp_c': None, 'precip_mm': None, 'snow_cm': None, 'wind_kmh': None}\n",
    "\n",
    "# Apply to accident data\n",
    "weather_data = accidents.apply(\n",
    "    lambda row: get_weather(row['latitude'], row['longitude'], row['date']), axis=1\n",
    ")\n",
    "\n",
    "# Convert weather data to DataFrame and merge\n",
    "weather_df = pd.json_normalize(weather_data)\n",
    "accidents_with_weather = pd.concat([accidents, weather_df], axis=1)\n",
    "\n",
    "print(accidents_with_weather)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aae610cb-6198-457c-b299-a1098cebc640",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Select distinct fiscal_yr values and order by descending\n",
    "distinct_fiscal_years = result_df.select(\"fiscal_yr\").distinct().orderBy(\"fiscal_yr\", ascending=False)\n",
    "\n",
    "# Display the distinct fiscal_yr values\n",
    "display(distinct_fiscal_years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c8ae381-b330-4264-a240-b4f9ae950f88",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "from meteostat import Point, Daily\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StructType, StructField, DoubleType, StringType\n",
    "\n",
    "# Function to fetch weather details\n",
    "def fetch_weather_for_row(lat, lon, date_str):\n",
    "    try:\n",
    "        location = Point(lat, lon)\n",
    "        date = datetime.strptime(date_str, \"%Y-%m-%d\")\n",
    "        data = Daily(location, date, date)\n",
    "        data = data.fetch()\n",
    "\n",
    "        if not data.empty:\n",
    "            temp_c = data['tavg'].iloc[0]\n",
    "            precip_mm = data['prcp'].iloc[0]\n",
    "            snow_cm = data['snow'].iloc[0]\n",
    "            wind_kmh = data['wspd'].iloc[0]\n",
    "\n",
    "            # Classify weather\n",
    "            if (temp_c is not None and (temp_c < 5 or temp_c > 30)) or \\\n",
    "               (precip_mm is not None and precip_mm >= 2) or \\\n",
    "               (snow_cm is not None and snow_cm > 0) or \\\n",
    "               (wind_kmh is not None and wind_kmh >= 40):\n",
    "                weather_summary = \"Bad\"\n",
    "            else:\n",
    "                weather_summary = \"Good\"\n",
    "\n",
    "            return (temp_c, precip_mm, snow_cm, wind_kmh, weather_summary)\n",
    "\n",
    "        else:\n",
    "            return (None, None, None, None, \"Unknown\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching weather for {lat}, {lon} on {date_str}: {e}\")\n",
    "        return (None, None, None, None, \"Unknown\")\n",
    "\n",
    "# Define the schema for the returned values\n",
    "schema = StructType([\n",
    "    StructField(\"temp_c\", DoubleType(), True),\n",
    "    StructField(\"precip_mm\", DoubleType(), True),\n",
    "    StructField(\"snow_cm\", DoubleType(), True),\n",
    "    StructField(\"wind_kmh\", DoubleType(), True),\n",
    "    StructField(\"weather_summary\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Register the UDF\n",
    "fetch_weather_udf = udf(fetch_weather_for_row, schema)\n",
    "\n",
    "# Apply the UDF to the dataframe\n",
    "result_df = result_df.withColumn(\n",
    "    \"weather_data\",\n",
    "    fetch_weather_udf(result_df[\"latitude\"], result_df[\"longitude\"], result_df[\"accident_dt\"])\n",
    ")\n",
    "\n",
    "# Split the struct column into separate columns\n",
    "result_df = result_df.select(\n",
    "    \"*\",\n",
    "    \"weather_data.temp_c\",\n",
    "    \"weather_data.precip_mm\",\n",
    "    \"weather_data.snow_cm\",\n",
    "    \"weather_data.wind_kmh\",\n",
    "    \"weather_data.weather_summary\"\n",
    ").drop(\"weather_data\")\n",
    "\n",
    "# Preview updated dataframe\n",
    "display(result_df)\n",
    "\n",
    "# Optional: Save to CSV\n",
    "# result_df.write.csv(\"accident_data_with_weather.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a4c4ca2-5f41-45c9-b2ca-72dce3fd3eb2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "latitude = 44.146667\n",
    "longitude = -72.481111\n",
    "date_str = \"2019-09-19\"\n",
    "\n",
    "# Open-Meteo API URL\n",
    "url = (\n",
    "    f\"https://archive-api.open-meteo.com/v1/archive?\"\n",
    "    f\"latitude={latitude}&longitude={longitude}\"\n",
    "    f\"&start_date={date_str}&end_date={date_str}\"\n",
    "    \"&daily=temperature_2m_max,precipitation_sum,snowfall_sum,windspeed_10m_max\"\n",
    "    \"&timezone=UTC\"\n",
    ")\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    data = response.json()\n",
    "    daily = data['daily']\n",
    "\n",
    "    temp_c = daily['temperature_2m_max'][0]\n",
    "    precip_mm = daily['precipitation_sum'][0]\n",
    "    snow_cm = daily['snowfall_sum'][0]\n",
    "    wind_kmh = daily['windspeed_10m_max'][0]\n",
    "\n",
    "    # Classify weather\n",
    "    if (temp_c < 5 or temp_c > 30) or precip_mm >= 2 or snow_cm > 0 or wind_kmh >= 40:\n",
    "        weather_summary = \"Bad\"\n",
    "    else:\n",
    "        weather_summary = \"Good\"\n",
    "\n",
    "    print(f\"Date: {date_str}\")\n",
    "    print(f\"Location: ({latitude}, {longitude})\")\n",
    "    print(f\"Max Temp (Â°C): {temp_c}\")\n",
    "    print(f\"Precipitation (mm): {precip_mm}\")\n",
    "    print(f\"Snowfall (cm): {snow_cm}\")\n",
    "    print(f\"Max Wind Speed (km/h): {wind_kmh}\")\n",
    "    print(f\"Weather Summary: {weather_summary}\")\n",
    "\n",
    "else:\n",
    "    print(f\"API Error: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c2e99c1-3eab-40d2-9b66-b9259bbd801f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install pandas tqdm requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5be587f1-710d-456f-99ab-bf6f95efb336",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "import requests\n",
    "import pandas as pd\n",
    "from tqdm import tqdm  # Progress bar\n",
    "\n",
    "# Assuming result_df is a PySpark DataFrame\n",
    "# Convert PySpark DataFrame to Pandas DataFrame\n",
    "result_pd_df = result_df.toPandas()\n",
    "\n",
    "# Function to fetch weather details for each row\n",
    "def fetch_weather(row):\n",
    "    lat = row['latitude']\n",
    "    lon = row['longitude']\n",
    "    date_str = row['accident_dt']\n",
    "\n",
    "    try:\n",
    "        # Open-Meteo API URL\n",
    "        url = (\n",
    "            f\"https://archive-api.open-meteo.com/v1/archive?\"\n",
    "            f\"latitude={lat}&longitude={lon}\"\n",
    "            f\"&start_date={date_str}&end_date={date_str}\"\n",
    "            \"&daily=temperature_2m_max,precipitation_sum,snowfall_sum,windspeed_10m_max\"\n",
    "            \"&timezone=UTC\"\n",
    "        )\n",
    "        response = requests.get(url)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            daily_data = response.json()['daily']\n",
    "            temp_c = daily_data['temperature_2m_max'][0]\n",
    "            precip_mm = daily_data['precipitation_sum'][0]\n",
    "            snow_cm = daily_data['snowfall_sum'][0]\n",
    "            wind_kmh = daily_data['windspeed_10m_max'][0]\n",
    "\n",
    "            # Classify weather\n",
    "            if (temp_c < 5 or temp_c > 30) or precip_mm >= 2 or snow_cm > 0 or wind_kmh >= 40:\n",
    "                weather_summary = \"Bad\"\n",
    "            else:\n",
    "                weather_summary = \"Good\"\n",
    "\n",
    "            return pd.Series([temp_c, precip_mm, snow_cm, wind_kmh, weather_summary])\n",
    "        else:\n",
    "            print(f\"API error {response.status_code} for {lat}, {lon} on {date_str}\")\n",
    "            return pd.Series([None, None, None, None, \"Unknown\"])\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching weather for {lat}, {lon} on {date_str}: {e}\")\n",
    "        return pd.Series([None, None, None, None, \"Unknown\"])\n",
    "\n",
    "# Apply fetch_weather function with progress bar\n",
    "tqdm.pandas(desc=\"Fetching Weather\")\n",
    "result_pd_df[['temp_c', 'precip_mm', 'snow_cm', 'wind_kmh', 'weather_summary']] = result_pd_df.progress_apply(\n",
    "    fetch_weather, axis=1\n",
    ")\n",
    "\n",
    "# Convert back to PySpark DataFrame if needed\n",
    "result_df = spark.createDataFrame(result_pd_df)\n",
    "\n",
    "# Preview updated DataFrame\n",
    "display(result_df)\n",
    "\n",
    "# Optionally save result\n",
    "# result_df.toPandas().to_csv(\"result_df_with_weather.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b2b6414-90b6-4b2f-a4c7-6c82e3501e4f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Get unique combinations\n",
    "unique_locations_df = result_df.select(\"latitude\", \"longitude\", \"accident_dt\").distinct()\n",
    "\n",
    "# Collect unique combinations to driver for API calls\n",
    "unique_locations = unique_locations_df.collect()\n",
    "\n",
    "\n",
    "\n",
    "weather_data = []\n",
    "\n",
    "for row in tqdm(unique_locations, desc=\"Fetching Weather\"):\n",
    "    lat = row['latitude']\n",
    "    lon = row['longitude']\n",
    "    date_str = row['accident_dt']\n",
    "\n",
    "    try:\n",
    "        # API Call\n",
    "        url = (\n",
    "            f\"https://archive-api.open-meteo.com/v1/archive?\"\n",
    "            f\"latitude={lat}&longitude={lon}\"\n",
    "            f\"&start_date={date_str}&end_date={date_str}\"\n",
    "            \"&daily=temperature_2m_max,precipitation_sum,snowfall_sum,windspeed_10m_max\"\n",
    "            \"&timezone=UTC\"\n",
    "        )\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            daily_data = response.json()['daily']\n",
    "            temp_c = daily_data['temperature_2m_max'][0]\n",
    "            precip_mm = daily_data['precipitation_sum'][0]\n",
    "            snow_cm = daily_data['snowfall_sum'][0]\n",
    "            wind_kmh = daily_data['windspeed_10m_max'][0]\n",
    "\n",
    "            # Classify weather\n",
    "            if (temp_c < 5 or temp_c > 30) or precip_mm >= 2 or snow_cm > 0 or wind_kmh >= 40:\n",
    "                weather_summary = \"Bad\"\n",
    "            else:\n",
    "                weather_summary = \"Good\"\n",
    "\n",
    "            # Append result\n",
    "            weather_data.append((lat, lon, date_str, temp_c, precip_mm, snow_cm, wind_kmh, weather_summary))\n",
    "        else:\n",
    "            # API failure\n",
    "            weather_data.append((lat, lon, date_str, None, None, None, None, \"Unknown\"))\n",
    "\n",
    "    except Exception as e:\n",
    "        # Exception case\n",
    "        weather_data.append((lat, lon, date_str, None, None, None, None, \"Unknown\"))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Prep Mining data",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
