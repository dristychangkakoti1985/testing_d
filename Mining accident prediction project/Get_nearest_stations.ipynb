{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd682495-c6cc-4d2d-9b5f-dc8cd73ceb0c",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1753307714464}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load the combined_mine_data table\n",
    "combined_mine_data_df = spark.table(\"default.combined_mine_data\")\n",
    "\n",
    "# Example query: Select specific columns and filter by a condition\n",
    "query = \"\"\"\n",
    "SELECT distinct accident_dt, latitude, longitude,current_mine_name, mine_id\n",
    "FROM default.combined_mine_data\n",
    "where fiscal_yr > 2020\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query\n",
    "result_df = spark.sql(query)\n",
    "\n",
    "# Display the result\n",
    "display(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "556154a9-c606-4b4b-9492-3625fca0a41e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(result_df.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "342754b5-002d-4dfa-abcb-a9c5b82e428f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install pandas tqdm requests\n",
    "%pip install meteostat geopy pandas tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20b515e3-d2ab-4936-ac2f-4e6522175aed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    " %restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60bc0efc-5635-4fbd-a9a3-acee09cf17ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Install dependencies if needed:\n",
    "# pip install meteostat geopy pandas tqdm numpy scipy\n",
    "\n",
    "from meteostat import Stations, Daily\n",
    "from scipy.spatial import cKDTree\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# --------------------------\n",
    "# CONFIGURATION\n",
    "# --------------------------\n",
    "OUTPUT_FILE = 'mine_accident_weather.csv'\n",
    "MISSING_LOG_FILE = 'missing_weather_log.csv'\n",
    "FALLBACK_NEAREST = 3  # Try up to 3 nearest stations\n",
    "MAX_WORKERS = 10      # Number of parallel API calls\n",
    "\n",
    "# --------------------------\n",
    "# STEP 1: PREPARE DATA\n",
    "# --------------------------\n",
    "# # result_df must include: current_mine_name, latitude, longitude, accident_dt\n",
    "# #result_df = pd.DataFrame()  \n",
    "# # Initialize result_df before use \n",
    "# required_cols = {'current_mine_name', 'latitude', 'longitude', 'accident_dt'}\n",
    "\n",
    "# if not required_cols.issubset(result_df.columns):\n",
    "#     raise ValueError(f\"result_df must have columns: {required_cols}\")\n",
    "\n",
    "result_df = result_df.toPandas()\n",
    "# print(result_df)  # Ensure result_df is loaded or initialized before this line\n",
    "# print(f\"‚úÖ Total accident rows: {len(result_df)}\")\n",
    "\n",
    "# Clean data types\n",
    "result_df['latitude'] = pd.to_numeric(result_df['latitude'], errors='coerce')\n",
    "result_df['longitude'] = pd.to_numeric(result_df['longitude'], errors='coerce')\n",
    "result_df['accident_dt'] = pd.to_datetime(result_df['accident_dt'], errors='coerce')\n",
    "\n",
    "# Drop rows with invalid coordinates or dates\n",
    "result_df = result_df.dropna(subset=['latitude', 'longitude', 'accident_dt']).reset_index(drop=True)\n",
    "\n",
    "# --------------------------\n",
    "# STEP 2: NEAREST WEATHER STATION\n",
    "# --------------------------\n",
    "print(\"üì° Fetching weather stations...\")\n",
    "stations = Stations().fetch().reset_index()\n",
    "stations['latitude'] = pd.to_numeric(stations['latitude'], errors='coerce')\n",
    "stations['longitude'] = pd.to_numeric(stations['longitude'], errors='coerce')\n",
    "stations = stations.dropna(subset=['latitude', 'longitude']).reset_index(drop=True)\n",
    "\n",
    "print(\"üèéÔ∏è Finding nearest stations for all accidents...\")\n",
    "station_coords = stations[['latitude', 'longitude']].to_numpy()\n",
    "mine_coords = result_df[['latitude', 'longitude']].to_numpy()\n",
    "\n",
    "# Build KDTree for fast lookup\n",
    "tree = cKDTree(station_coords)\n",
    "distances, indices = tree.query(mine_coords, k=FALLBACK_NEAREST)\n",
    "\n",
    "# Add nearest station info to result_df\n",
    "result_df['station_id'] = stations.iloc[indices[:, 0]]['id'].values\n",
    "result_df['station_name'] = stations.iloc[indices[:, 0]]['name'].values\n",
    "result_df['distance_km'] = distances[:, 0]\n",
    "\n",
    "# --------------------------\n",
    "# STEP 3: FETCH WEATHER DATA (BATCHED)\n",
    "# --------------------------\n",
    "print(\"üå§Ô∏è Fetching weather data in parallel...\")\n",
    "\n",
    "# Group by station to avoid redundant API calls\n",
    "station_groups = result_df.groupby('station_id')['accident_dt'].apply(lambda x: sorted(set(x))).to_dict()\n",
    "\n",
    "def fetch_weather_for_station(station_id, dates):\n",
    "    \"\"\"\n",
    "    Fetch weather data for a station and filter for accident dates\n",
    "    \"\"\"\n",
    "    try:\n",
    "        start_date = min(dates)\n",
    "        end_date = max(dates)\n",
    "        data = Daily(station_id, start_date, end_date).fetch().reset_index()\n",
    "        if data.empty:\n",
    "            return station_id, None\n",
    "        else:\n",
    "            # Filter only accident dates\n",
    "            filtered = data[data['time'].isin(dates)].copy()\n",
    "            filtered['station_id'] = station_id\n",
    "            return station_id, filtered\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error fetching data for station {station_id}: {e}\")\n",
    "        return station_id, None\n",
    "\n",
    "# Parallelize API calls\n",
    "station_weather = {}\n",
    "with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "    futures = {executor.submit(fetch_weather_for_station, sid, dates): sid for sid, dates in station_groups.items()}\n",
    "    for future in tqdm(as_completed(futures), total=len(futures), desc=\"Downloading weather\"):\n",
    "        sid, weather = future.result()\n",
    "        station_weather[sid] = weather\n",
    "\n",
    "# --------------------------\n",
    "# STEP 4: MERGE WEATHER DATA\n",
    "# --------------------------\n",
    "print(\"üîÑ Merging weather data with accidents...\")\n",
    "weather_df = pd.concat([df for df in station_weather.values() if df is not None], ignore_index=True)\n",
    "\n",
    "# Merge weather details into result_df\n",
    "final_df = result_df.merge(\n",
    "    weather_df,\n",
    "    how='left',\n",
    "    left_on=['station_id', 'accident_dt'],\n",
    "    right_on=['station_id', 'time']\n",
    ").drop(columns=['time'])\n",
    "\n",
    "print(final_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56ecc125-6dcd-4652-899f-142ce2e31883",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1753308719405}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Convert columns with null values to 0\n",
    "final_df = final_df.fillna(0)\n",
    "final_df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e519ddf-ef05-4b7e-9af0-54b7dc072ea8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# STEP 5: CLASSIFY WEATHER\n",
    "# --------------------------\n",
    "def classify_weather(row):\n",
    "    \"\"\"Classify weather as Good, Moderate, or Bad\"\"\"\n",
    "    if pd.isna(row['tavg']):\n",
    "        return \"Unknown Weather\"\n",
    "    if 10 <= row['tavg'] <= 30 and row['prcp'] < 20:  # Adjust thresholds if needed\n",
    "        return \"Good Weather\"\n",
    "    elif row['tavg'] < -10 or row['tavg'] > 40 or row['prcp'] > 50:\n",
    "        return \"Bad Weather\"\n",
    "    else:\n",
    "        return \"Moderate Weather\"\n",
    "\n",
    "final_df['weather_summary'] = final_df.apply(classify_weather, axis=1)\n",
    "\n",
    "# --------------------------\n",
    "# STEP 6: SAVE OUTPUT\n",
    "# --------------------------\n",
    "# final_df.to_csv(OUTPUT_FILE, index=False)\n",
    "# print(f\"‚úÖ Accident weather data saved to {OUTPUT_FILE}\")\n",
    "\n",
    "# # Log missing weather\n",
    "# missing_weather = final_df[final_df['tavg'].isna()][['current_mine_name', 'accident_dt']]\n",
    "# if not missing_weather.empty:\n",
    "#     missing_weather.to_csv(MISSING_LOG_FILE, index=False)\n",
    "#     print(f\"‚ö†Ô∏è Logged {len(missing_weather)} accident rows with missing weather to '{MISSING_LOG_FILE}'.\")\n",
    "\n",
    "# Preview\n",
    "print(final_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ccc2d92-7795-42d8-9b41-64b8c55c9478",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Convert Pandas DataFrame to Spark DataFrame\n",
    "spark_df = spark.createDataFrame(final_df)\n",
    "# Drop the existing table if it exists\n",
    "spark.sql(\"DROP TABLE IF EXISTS default.mine_accident_weather\")\n",
    "\n",
    "# Save spark_df as a Delta table\n",
    "spark_df.write.mode(\"overwrite\").saveAsTable(\"default.mine_accident_weather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79944d74-038a-4848-b77f-8141ae5d540b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(final_df['tavg'].notna().sum(), \"rows have weather data\")\n",
    "print(final_df['tavg'].isna().sum(), \"rows missing weather data\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Get_nearest_stations",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
