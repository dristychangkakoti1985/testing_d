{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c46f1f5-7203-4058-8b4e-c86144be3282",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Path to your extracted file\n",
    "file_path = \"/Volumes/workspace/default/miningaccidents/ContractorProdYearly.txt\"  # Replace with your local or cloud path\n",
    "\n",
    "\n",
    "# Load the file using pipe delimiter and quoted values\n",
    "df = pd.read_csv(\n",
    "    file_path,\n",
    "    sep=\"|\",\n",
    "    quotechar='\"',\n",
    "    encoding=\"latin1\",  # Use 'latin1' encoding to handle decoding errors\n",
    "    dtype=str  # Start with all fields as strings\n",
    ")\n",
    "\n",
    "# Strip whitespace from column names and convert to lowercase\n",
    "df.columns = [col.strip().lower().replace(\" \", \"_\") for col in df.columns]\n",
    "\n",
    "# # Convert relevant date fields\n",
    "# df['accident_dt'] = pd.to_datetime(df['accident_dt'], errors='coerce')\n",
    "# df['return_to_work_dt'] = pd.to_datetime(df['return_to_work_dt'], errors='coerce')\n",
    "\n",
    "# # Drop rows missing crucial information (e.g., mine_id or accident_dt)\n",
    "# df = df.dropna(subset=[\"mine_id\", \"accident_dt\"])\n",
    "\n",
    "# # Preview cleaned data\n",
    "# # print(df.head())\n",
    "\n",
    "# Convert Pandas DataFrame to Spark DataFrame\n",
    "spark_df = spark.createDataFrame(df)\n",
    "\n",
    "# Save as Delta table in the default workspace\n",
    "table_name = \"contractor_prod_yearly\"\n",
    "spark_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(table_name)\n",
    "\n",
    "print(f\"ðŸŽ‰ Data saved as Delta table: {table_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "916957eb-c219-4b73-93a2-92f10a3568bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Path to your extracted file\n",
    "file_path = \"/Volumes/workspace/default/miningaccidents/MinesProdYearly.txt\"  # Replace with your local or cloud path\n",
    "\n",
    "\n",
    "# Load the file using pipe delimiter and quoted values\n",
    "df = pd.read_csv(\n",
    "    file_path,\n",
    "    sep=\"|\",\n",
    "    quotechar='\"',\n",
    "    encoding=\"latin1\",  # Use 'latin1' encoding to handle decoding errors\n",
    "    dtype=str  # Start with all fields as strings\n",
    ")\n",
    "\n",
    "# Strip whitespace from column names and convert to lowercase\n",
    "df.columns = [col.strip().lower().replace(\" \", \"_\") for col in df.columns]\n",
    "\n",
    "# # Convert relevant date fields\n",
    "# df['accident_dt'] = pd.to_datetime(df['accident_dt'], errors='coerce')\n",
    "# df['return_to_work_dt'] = pd.to_datetime(df['return_to_work_dt'], errors='coerce')\n",
    "\n",
    "# # Drop rows missing crucial information (e.g., mine_id or accident_dt)\n",
    "# df = df.dropna(subset=[\"mine_id\", \"accident_dt\"])\n",
    "\n",
    "# # Preview cleaned data\n",
    "# # print(df.head())\n",
    "\n",
    "# Convert Pandas DataFrame to Spark DataFrame\n",
    "spark_df = spark.createDataFrame(df)\n",
    "\n",
    "# Save as Delta table in the default workspace\n",
    "table_name = \"mines_prod_yearly\"\n",
    "spark_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(table_name)\n",
    "\n",
    "print(f\"ðŸŽ‰ Data saved as Delta table: {table_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed879b02-8aca-4810-8570-073aba353777",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Path to your extracted file\n",
    "file_path = \"/Volumes/workspace/default/miningaccidents/PersonalHealthSamples.txt\"  # Replace with your local or cloud path\n",
    "\n",
    "\n",
    "# Load the file using pipe delimiter and quoted values\n",
    "df = pd.read_csv(\n",
    "    file_path,\n",
    "    sep=\"|\",\n",
    "    quotechar='\"',\n",
    "    encoding=\"latin1\",  # Use 'latin1' encoding to handle decoding errors\n",
    "    dtype=str  # Start with all fields as strings\n",
    ")\n",
    "\n",
    "# Strip whitespace from column names and convert to lowercase\n",
    "df.columns = [col.strip().lower().replace(\" \", \"_\") for col in df.columns]\n",
    "\n",
    "# # Convert relevant date fields\n",
    "# df['accident_dt'] = pd.to_datetime(df['accident_dt'], errors='coerce')\n",
    "# df['return_to_work_dt'] = pd.to_datetime(df['return_to_work_dt'], errors='coerce')\n",
    "\n",
    "# # Drop rows missing crucial information (e.g., mine_id or accident_dt)\n",
    "# df = df.dropna(subset=[\"mine_id\", \"accident_dt\"])\n",
    "\n",
    "# # Preview cleaned data\n",
    "# # print(df.head())\n",
    "\n",
    "# Convert Pandas DataFrame to Spark DataFrame\n",
    "spark_df = spark.createDataFrame(df)\n",
    "\n",
    "# Save as Delta table in the default workspace\n",
    "table_name = \"health_samples\"\n",
    "spark_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(table_name)\n",
    "\n",
    "print(f\"ðŸŽ‰ Data saved as Delta table: {table_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f228eb4d-1ef3-4aa4-b352-b9affa202b90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Analyse_accidents_data",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
