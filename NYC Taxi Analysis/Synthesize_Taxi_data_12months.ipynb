{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4df650e8-b76e-401b-b7c0-969f767892b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import StructType, StructField, TimestampType, DoubleType, StringType\n",
    "from pyspark.sql.window import Window\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Define the schema for the synthesized data\n",
    "synthesized_schema = StructType([\n",
    "    StructField(\"tpep_pickup_datetime\", TimestampType(), True),\n",
    "    StructField(\"tpep_dropoff_datetime\", TimestampType(), True),\n",
    "    StructField(\"trip_distance\", DoubleType(), True),\n",
    "    StructField(\"fare_amount\", DoubleType(), True),\n",
    "    StructField(\"pickup_zip\", StringType(), True),\n",
    "    StructField(\"dropoff_zip\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Read the original data from the Delta table\n",
    "try:\n",
    "    df_original = spark.read.format(\"delta\").table(\"samples.nyctaxi.trips\")\n",
    "except Exception as e:\n",
    "    print(f\"Error reading Delta table: {e}. Please ensure you have access to 'samples.nyctaxi.trips'.\")\n",
    "    exit()\n",
    "\n",
    "# Extract useful features and convert to Pandas for pattern analysis\n",
    "df_original_pd = df_original.select(\n",
    "    F.hour(F.col(\"tpep_pickup_datetime\")).alias(\"pickup_hour\"),\n",
    "    F.dayofweek(F.col(\"tpep_pickup_datetime\")).alias(\"pickup_day_of_week\"),\n",
    "    (F.col(\"tpep_dropoff_datetime\").cast(\"long\") - F.col(\"tpep_pickup_datetime\").cast(\"long\")).alias(\"trip_duration_seconds\"),\n",
    "    F.col(\"trip_distance\"),\n",
    "    F.col(\"fare_amount\"),\n",
    "    F.col(\"pickup_zip\"),\n",
    "    F.col(\"dropoff_zip\")\n",
    ").toPandas()\n",
    "\n",
    "# Group original data for faster lookup during synthesis\n",
    "data_by_hour = df_original_pd.groupby(['pickup_day_of_week', 'pickup_hour']).apply(\n",
    "    lambda x: x.to_dict('records')\n",
    ").reset_index(name='trips_data')\n",
    "\n",
    "# Define the start and end date for the new dataset\n",
    "start_date = datetime(2016, 1, 1)\n",
    "end_date = datetime(2016, 12, 31)\n",
    "\n",
    "# Create a new list to hold the synthesized records\n",
    "new_records = []\n",
    "current_date = start_date\n",
    "\n",
    "while current_date <= end_date:\n",
    "    day_of_week = current_date.weekday()\n",
    "    hour = current_date.hour\n",
    "\n",
    "    relevant_trips_df = data_by_hour[\n",
    "        (data_by_hour['pickup_day_of_week'] == day_of_week) &\n",
    "        (data_by_hour['pickup_hour'] == hour)\n",
    "    ]\n",
    "\n",
    "    if not relevant_trips_df.empty:\n",
    "        relevant_trips = relevant_trips_df['trips_data'].iloc[0]\n",
    "        \n",
    "        num_trips_to_generate = random.randint(50, 150)\n",
    "        \n",
    "        sampled_trips = random.choices(relevant_trips, k=min(num_trips_to_generate, len(relevant_trips)))\n",
    "\n",
    "        for row in sampled_trips:\n",
    "            new_trip_distance = row['trip_distance'] * (1 + random.gauss(0, 0.1))\n",
    "            new_fare_amount = row['fare_amount'] * (1 + random.gauss(0, 0.1))\n",
    "\n",
    "            new_pickup_datetime = current_date + timedelta(minutes=random.randint(0, 60))\n",
    "            new_dropoff_datetime = new_pickup_datetime + timedelta(seconds=row['trip_duration_seconds'])\n",
    "\n",
    "            new_records.append({\n",
    "                'tpep_pickup_datetime': new_pickup_datetime,\n",
    "                'tpep_dropoff_datetime': new_dropoff_datetime,\n",
    "                'trip_distance': new_trip_distance,\n",
    "                'fare_amount': new_fare_amount,\n",
    "                'pickup_zip': row['pickup_zip'],\n",
    "                'dropoff_zip': row['dropoff_zip']\n",
    "            })\n",
    "    \n",
    "    current_date += timedelta(hours=1)\n",
    "\n",
    "# Create a Spark DataFrame from the synthesized records\n",
    "df_synthesized_spark = spark.createDataFrame(new_records, schema=synthesized_schema)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a9ca8c9-67ce-4f8f-8468-3caf0772d419",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_synthesized_spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c06b46f-267e-4d37-8096-0e31bcb775c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "%python\n",
    "df_synthesized_spark.write.csv(\n",
    "    path=\"/Volumes/workspace/default/nyctaxi/nyctaxi_synthesized.csv\",\n",
    "    header=True,\n",
    "    mode=\"overwrite\"\n",
    ")\n",
    "\n",
    "print(f\"Synthesized data with {df_synthesized_spark.count()} records for 12 months has been saved to '{output_path}'.\")\n",
    "# Save the synthesized data to a new Delta table\n",
    "# output_path = \"/Volumes/workspace/default/nyctaxi\"\n",
    "# df_synthesized_spark.write.format(\"delta\").mode(\"overwrite\").save(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42887182-32c4-4d55-80ff-66525148d2ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "df_synthesized_spark.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"workspace.default.nyctaxi\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Synthesize_Taxi_data_12months",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
