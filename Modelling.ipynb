{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0d647f5-e333-45cc-b2c2-bd78566da9ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, col\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "# Start Spark session (Databricks auto-creates this for you)\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Load the table from Unity Catalog with a filter for fiscal year > 2020\n",
    "df = spark.read.table(\"workspace.default.preprossesed_mines_data\").filter(\"fiscal_yr > 2020\")\n",
    "\n",
    "# # Preview data\n",
    "# df = df.select(\"mine_id\",\"current_mine_type\", \"primary_sic\",\"subunit\", \"weather_summary\", \"shift_type\",\"mining_equip\",\n",
    "#           \"occupation\", \"time_diff\", \"narrative\",\"last_maintenance_dt\",\"tavg\",\"prcp\",\"snow\").show(5)\n",
    "\n",
    "# Select the desired columns\n",
    "selected_columns_df = df.select(\n",
    "    \"mine_id\",\n",
    "    \"current_mine_type\",\n",
    "    \"primary_sic\",\n",
    "    \"subunit\",\n",
    "    \"weather_summary\",\n",
    "    \"shift_type\",\n",
    "    \"mining_equip\",\n",
    "    \"occupation\",\n",
    "    \"time_diff\",\n",
    "    \"narrative\",\n",
    "    \"last_maintenance_dt\",\n",
    "    \"accident_dt\",\n",
    "    \"tavg\",\n",
    "    \"prcp\",\n",
    "    \"snow\"\n",
    ")\n",
    "display(selected_columns_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2eda658-a06f-4997-9bfe-519356a49b7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit, rand\n",
    "from pyspark.sql.functions import col, datediff, row_number\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "\n",
    "# Positive examples\n",
    "accidents = selected_columns_df.withColumn(\"accident\", lit(1))\n",
    "\n",
    "# Negative examples - shuffle/corrupt features slightly\n",
    "non_accidents = selected_columns_df.withColumn(\"accident\", lit(0))\n",
    "# Combine both\n",
    "balanced_df = accidents.union(non_accidents)\n",
    "balanced_df= balanced_df.withColumn(\"days_since_maint\", datediff(col(\"accident_dt\"), col(\"last_maintenance_dt\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b31071f8-e582-4155-baec-4b9ecc65051e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "print(balanced_df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "354f2c10-3d36-4894-9e79-b82a5a7dfef9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "string_cols = [f.name for f in balanced_df.schema.fields if str(f.dataType) == 'StringType()']\n",
    "print(string_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "474f32a3-36a8-46c6-928c-89de43ab1de6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define a function to label encode with Pandas\n",
    "def encode_pandas(df_iter):\n",
    "    for pdf in df_iter:\n",
    "        for col_name in string_cols:\n",
    "            pdf[col_name + \"_encoded\"] = pdf[col_name].astype(\"category\").cat.codes\n",
    "        yield pdf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d3253fe-9ea7-4403-bccb-e2a8722db108",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructField, IntegerType, StructType\n",
    "\n",
    "# Create a new schema by adding encoded columns to the existing schema\n",
    "new_schema = balanced_df.schema\n",
    "for col_name in string_cols:\n",
    "    new_schema = new_schema.add(StructField(f\"{col_name}_encoded\", IntegerType()))\n",
    "\n",
    "# Apply encoding\n",
    "df_encoded = balanced_df.mapInPandas(encode_pandas, schema=new_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5e6cba1-2e59-43f6-8068-b7eb98cc76cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# List of all encoded columns\n",
    "encoded_cols = [col_name + \"_encoded\" for col_name in string_cols]\n",
    "\n",
    "# Numerical columns to include\n",
    "numeric_cols = ['time_diff', 'tavg', 'prcp', 'snow', 'days_since_maint', 'equipment_age_yrs', 'failure_probability']\n",
    "\n",
    "# Final features to include in the model\n",
    "feature_cols = encoded_cols + numeric_cols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5cc100e5-c210-49fd-94b4-55a81af35e5f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def assemble_features(pdf: pd.DataFrame) -> pd.DataFrame:\n",
    "    # List your feature columns here\n",
    "    feature_cols = ['mine_id', 'current_mine_type', 'primary_sic', 'subunit',\n",
    "                    'weather_summary', 'shift_type', 'mining_equip', 'occupation',\n",
    "                    'time_diff', 'tavg', 'prcp', 'snow',\n",
    "                    'days_since_maint', 'equipment_age_yrs', 'failure_probability']\n",
    "    \n",
    "    # Stack features into a vector\n",
    "    pdf['features'] = pdf[feature_cols].astype(float).values.tolist()\n",
    "    \n",
    "    return pdf[['features', 'accident']]\n",
    "\n",
    "assembled_df = df_encoded.mapInPandas(assemble_features, schema=\"features array<double>, accident int\")\n",
    "assembled_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "abe1a5cf-23e3-4b26-8566-e2df48712069",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Use the dataframe returned from mapInPandas\n",
    "# Example: `assembled_df` has `features` and `accident` columns\n",
    "# Let's cache it for performance\n",
    "# assembled_df = assembled_df.cache()\n",
    "\n",
    "# ----------------------\n",
    "# Step 1: Train/test split\n",
    "# ----------------------\n",
    "train_df, test_df = assembled_df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# ----------------------\n",
    "# Step 2: Define Model\n",
    "# ----------------------\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"accident\", maxIter=10)\n",
    "\n",
    "# ----------------------\n",
    "# Step 3: Train Model\n",
    "# ----------------------\n",
    "lr_model = lr.fit(train_df)\n",
    "\n",
    "# ----------------------\n",
    "# Step 4: Predictions\n",
    "# ----------------------\n",
    "predictions = lr_model.transform(test_df)\n",
    "predictions.select(\"accident\", \"probability\", \"prediction\").show(10, truncate=False)\n",
    "\n",
    "# ----------------------\n",
    "# Step 5: Evaluate\n",
    "# ----------------------\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"accident\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
    "roc_auc = evaluator.evaluate(predictions)\n",
    "print(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "224bc205-8174-411a-bf34-7ad982cc973b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(labelCol=\"accident\", featuresCol=\"features\", numTrees=100)\n",
    "pipeline = Pipeline(stages=[assembler, rf])\n",
    "\n",
    "model = pipeline.fit(train_data)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Modelling",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
