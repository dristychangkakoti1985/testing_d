{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0d647f5-e333-45cc-b2c2-bd78566da9ab",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1753743018915}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, col\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "# Start Spark session (Databricks auto-creates this for you)\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Load the table from Unity Catalog with a filter for fiscal year > 2020\n",
    "df = spark.read.table(\"workspace.default.preprossesed_mines_data\").filter(\"fiscal_yr > 2020\").limit(1000)\n",
    "\n",
    "# # Preview data\n",
    "# df = df.select(\"mine_id\",\"current_mine_type\", \"primary_sic\",\"subunit\", \"weather_summary\", \"shift_type\",\"mining_equip\",\n",
    "#           \"occupation\", \"time_diff\", \"narrative\",\"last_maintenance_dt\",\"tavg\",\"prcp\",\"snow\").show(5)\n",
    "\n",
    "# Select the desired columns\n",
    "selected_columns_df = df.select(\n",
    "    \"mine_id\",\n",
    "    \"current_mine_type\",\n",
    "    \"primary_sic\",\n",
    "    \"subunit\",\n",
    "    \"weather_summary\",\n",
    "    \"shift_type\",\n",
    "    \"mining_equip\",\n",
    "    \"occupation\",\n",
    "    \"time_diff\",\n",
    "    \"narrative\",\n",
    "    \"last_maintenance_dt\",\n",
    "    \"accident_dt\",\n",
    "    \"tavg\",\n",
    "    \"prcp\",\n",
    "    \"snow\",'equipment_age_yrs', 'failure_probability'\n",
    ")\n",
    "display(selected_columns_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2eda658-a06f-4997-9bfe-519356a49b7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit, rand\n",
    "from pyspark.sql.functions import col, datediff, row_number\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "\n",
    "# Positive examples\n",
    "accidents = selected_columns_df.withColumn(\"accident\", lit(1))\n",
    "\n",
    "# Negative examples - shuffle/corrupt features slightly\n",
    "non_accidents = selected_columns_df.withColumn(\"accident\", lit(0))\n",
    "# Combine both\n",
    "balanced_df = accidents.union(non_accidents)\n",
    "balanced_df= balanced_df.withColumn(\"days_since_maint\", datediff(col(\"accident_dt\"), col(\"last_maintenance_dt\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b31071f8-e582-4155-baec-4b9ecc65051e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Count class distribution\n",
    "class_counts = balanced_df.groupBy(\"accident\").count().collect()\n",
    "counts = {row['accident']: row['count'] for row in class_counts}\n",
    "\n",
    "# Desired total sample size\n",
    "sample_size = 1000\n",
    "\n",
    "# Stratified sampling fractions\n",
    "fractions = {\n",
    "    0: min(1.0, sample_size * 0.5 / counts[0]),\n",
    "    1: min(1.0, sample_size * 0.5 / counts[1])\n",
    "}\n",
    "\n",
    "sampled_df = balanced_df.stat.sampleBy(\"accident\", fractions=fractions, seed=42)\n",
    "\n",
    "display(sampled_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7495e17f-9eb6-4829-8f3e-566f871a72ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ---------------New Approach----------------\n",
    "# 1. Import libraries\n",
    "import pyspark.pandas as ps\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# # Load the table from Unity Catalog with a filter for fiscal year > 2020\n",
    "# df = spark.read.table(\"workspace.default.preprossesed_mines_data\").filter(\"fiscal_yr > 2020\")\n",
    "\n",
    "# 3. Convert Spark → Pandas-on-Spark → Pandas\n",
    "df_ps = sampled_df.to_pandas_on_spark()\n",
    "df_pd = df_ps.to_pandas()\n",
    "\n",
    "# 4. Drop nulls or fill if needed\n",
    "df_pd = df_pd.dropna()\n",
    "\n",
    "# 5. Encode categorical variables\n",
    "categorical_cols = ['mine_id', 'current_mine_type', 'primary_sic', 'subunit',\n",
    "                    'weather_summary', 'shift_type', 'mining_equip', 'occupation',\n",
    "                    'time_diff', 'tavg', 'prcp', 'snow',\n",
    "                    'days_since_maint', 'equipment_age_yrs', 'failure_probability']\n",
    "\n",
    "df_encoded = pd.get_dummies(df_pd, columns=categorical_cols, drop_first=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01b4550e-576b-4688-a80b-8e892d901235",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# 6. Define features and label\n",
    "X = df_encoded.drop(columns=['accident'])  # assuming 'accident' is the binary label\n",
    "y = df_encoded['accident']\n",
    "\n",
    "# # 7. Train/test split\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # 8. Train logistic regression model\n",
    "# model = LogisticRegression(max_iter=1000)\n",
    "# model.fit(X_train, y_train)\n",
    "\n",
    "# Convert text data to numerical features\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_transformed = vectorizer.fit_transform(X['narrative'])  # replace 'text_column' with the actual column name containing text data\n",
    "\n",
    "# 7. Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_transformed, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 8. Train logistic regression model\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 9. Predict & evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Modelling",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
