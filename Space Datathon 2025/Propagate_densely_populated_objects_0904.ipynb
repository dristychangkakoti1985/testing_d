{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b6e600b-adfd-41fb-ae6c-814f7de9f93a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "%pip install sgp4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07018e66-f4b7-49b9-ad04-b14afa8e8f45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read the data from the Delta table\n",
    "df = spark.read.table(\"workspace.default.objects_in_LEO_0904\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06ab99d1-87c7-4aa4-90be-115f72671c42",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"TCA\":121},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1756925976436}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from sgp4.api import Satrec, WGS72\n",
    "\n",
    "# --- Position helper ---\n",
    "def get_position(tle1, tle2, when=\"2025-09-10T00:00:00\"):\n",
    "    sat = Satrec.twoline2rv(tle1, tle2, WGS72)\n",
    "    ts = datetime.fromisoformat(when)\n",
    "    e, r, v = sat.sgp4(ts.year, ts.timetuple().tm_yday + ts.hour/24 + ts.minute/(24*60) + ts.second/(24*3600))\n",
    "    if e != 0:\n",
    "        return None\n",
    "    return np.array(r)  # (x,y,z) in km\n",
    "\n",
    "def distance_km(p1, p2):\n",
    "    return np.linalg.norm(p1 - p2)\n",
    "\n",
    "# --- Miss distance sweep ---\n",
    "def miss_distance_sweep(tle1_a, tle2_a, tle1_b, tle2_b,\n",
    "                        center_dt=\"2025-09-10T00:00:00\", hours=24, step_min=10):\n",
    "    center = datetime.fromisoformat(center_dt)\n",
    "    t = center - timedelta(hours=hours)\n",
    "    t_end = center + timedelta(hours=hours)\n",
    "\n",
    "    d_min, t_min = float(\"inf\"), None\n",
    "    while t <= t_end:\n",
    "        ts = t.strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "        pa = get_position(tle1_a, tle2_a, ts)\n",
    "        pb = get_position(tle1_b, tle2_b, ts)\n",
    "        if pa is None or pb is None:\n",
    "            t += timedelta(minutes=step_min)\n",
    "            continue\n",
    "        d = distance_km(pa, pb)\n",
    "        if d < d_min:\n",
    "            d_min, t_min = d, t\n",
    "        t += timedelta(minutes=step_min)\n",
    "    return d_min, t_min\n",
    "\n",
    "# --- Build nearest neighbor pairs ---\n",
    "def build_candidate_pairs(df, top_n=50):\n",
    "    # quick proxy distance metric: altitude + inclination differences\n",
    "    df[\"prox_score\"] = (df[\"ALTITUDE_KM\"].rank() + df[\"INCLINATION_p\"].rank())\n",
    "    # take top_n nearest \"neighbors\" by sorting\n",
    "    top_df = df.nsmallest(top_n, \"prox_score\")\n",
    "    pairs = []\n",
    "    for i, row_a in top_df.iterrows():\n",
    "        for j, row_b in top_df.iterrows():\n",
    "            if i >= j:  # avoid duplicate/self\n",
    "                continue\n",
    "            pairs.append((row_a[\"OBJECT_ID\"], row_b[\"OBJECT_ID\"],\n",
    "                          row_a[\"TLE_LINE1\"], row_a[\"TLE_LINE2\"],\n",
    "                          row_b[\"TLE_LINE1\"], row_b[\"TLE_LINE2\"]))\n",
    "    return pairs\n",
    "\n",
    "# --- Run miss distance on candidate pairs ---\n",
    "pairs = build_candidate_pairs(df, top_n=30)   # adjust N\n",
    "results = []\n",
    "\n",
    "for obj_a, obj_b, tle1_a, tle2_a, tle1_b, tle2_b in pairs:\n",
    "    d_min, t_min = miss_distance_sweep(tle1_a, tle2_a, tle1_b, tle2_b,\n",
    "                                       center_dt=\"2025-09-10T00:00:00\",\n",
    "                                       hours=12, step_min=15)\n",
    "    results.append((obj_a, obj_b, d_min, t_min))\n",
    "\n",
    "conjunctions_df = pd.DataFrame(results, columns=[\"Object_A\", \"Object_B\", \"Miss_Distance_km\", \"TCA\"])\n",
    "\n",
    "# Display the DataFrame as a table\n",
    "display(conjunctions_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "535aa178-b5d2-450c-8951-316a628f78c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_position(tle1, tle2, when=\"2025-09-10T00:00:00\"):\n",
    "    sat = Satrec.twoline2rv(tle1, tle2, WGS72)\n",
    "    ts = datetime.fromisoformat(when)\n",
    "    jd = ts.timetuple().tm_yday + ts.hour/24 + ts.minute/(24*60) + ts.second/(24*3600)\n",
    "    e, r, v = sat.sgp4(ts.year, jd)\n",
    "    if e != 0:   # SGP4 error\n",
    "        return None\n",
    "    return np.array(r)  # (x,y,z) in km\n",
    "\n",
    "def miss_distance_sweep(tle1_a, tle2_a, tle1_b, tle2_b,\n",
    "                        center_dt=\"2025-09-10T00:00:00\", hours=24, step_min=10):\n",
    "    center = datetime.fromisoformat(center_dt)\n",
    "    t = center - timedelta(hours=hours)\n",
    "    t_end = center + timedelta(hours=hours)\n",
    "\n",
    "    d_min, t_min = float(\"inf\"), None\n",
    "    while t <= t_end:\n",
    "        ts = t.strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "        pa = get_position(tle1_a, tle2_a, ts)\n",
    "        pb = get_position(tle1_b, tle2_b, ts)\n",
    "        if pa is None or pb is None:   # skip bad propagation\n",
    "            t += timedelta(minutes=step_min)\n",
    "            continue\n",
    "        d = distance_km(pa, pb)\n",
    "        if d < d_min:\n",
    "            d_min, t_min = d, t\n",
    "        t += timedelta(minutes=step_min)\n",
    "\n",
    "    if d_min == float(\"inf\"):  # never found valid distance\n",
    "        return None, None\n",
    "    return d_min, t_min\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38b5b1a3-6935-4f94-a735-6a3ee255c75d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "for obj_a, obj_b, tle1_a, tle2_a, tle1_b, tle2_b in pairs:\n",
    "    d_min, t_min = miss_distance_sweep(tle1_a, tle2_a, tle1_b, tle2_b,\n",
    "                                       center_dt=\"2025-09-10T00:00:00\",\n",
    "                                       hours=12, step_min=15)\n",
    "    if d_min is not None:   # keep only valid cases\n",
    "        results.append((obj_a, obj_b, d_min, t_min))\n",
    "\n",
    "conjunctions_df = pd.DataFrame(results, columns=[\"Object_A\", \"Object_B\", \"Miss_Distance_km\", \"TCA\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6475b340-178a-460e-87a0-c3cbbef82153",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(conjunctions_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b10a0aef-8acf-43c3-8c10-24732f529054",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Filter the DataFrame for Object_A = '2022-064F'\n",
    "filtered_df = conjunctions_df[conjunctions_df[\"Object_A\"] == \"2022-064F\"]\n",
    "\n",
    "# Display the filtered DataFrame\n",
    "display(filtered_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "864f7882-f9ad-4398-bd93-e57abc342a87",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1756940852280}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read the data from the Delta table\n",
    "leo_df = spark.read.table(\"workspace.default.objects_in_LEO_0904\").toPandas()\n",
    "\n",
    "# Join filtered_df with leo_df to get additional information for Object_A\n",
    "joined_df = conjunctions_df.merge(leo_df[[\"OBJECT_ID\", \"OBJECT_TYPE\"]], \n",
    "                              left_on=\"Object_A\", \n",
    "                              right_on=\"OBJECT_ID\", \n",
    "                              how=\"left\").rename(columns={\"OBJECT_TYPE\": \"Object_Type_A\"}).drop(columns=[\"OBJECT_ID\"])\n",
    "\n",
    "# Join again to get additional information for Object_B\n",
    "joined_df = joined_df.merge(leo_df[[\"OBJECT_ID\", \"OBJECT_TYPE\"]], \n",
    "                            left_on=\"Object_B\", \n",
    "                            right_on=\"OBJECT_ID\", \n",
    "                            how=\"left\").rename(columns={\"OBJECT_TYPE\": \"Object_Type_B\"}).drop(columns=[\"OBJECT_ID\"])\n",
    "\n",
    "# Display the joined DataFrame\n",
    "display(joined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28fdc570-834a-4c0f-bd82-f131946510f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define a threshold for high-risk conjunctions (e.g., < 10 km)\n",
    "HIGH_RISK_THRESHOLD_KM = 10\n",
    "\n",
    "# Filter for high-risk conjunctions\n",
    "high_risk_df = joined_df[joined_df[\"Miss_Distance_km\"] < HIGH_RISK_THRESHOLD_KM]\n",
    "\n",
    "# Display the high-risk conjunctions\n",
    "display(high_risk_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e5c02e4-e1f3-4924-b052-d29ea8d4e9d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Collision probability\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.linalg as la\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "# ----------------------------\n",
    "# Helper: Monte-Carlo Pc estimator\n",
    "# ----------------------------\n",
    "def estimate_pc_monte_carlo(\n",
    "    rel_pos_km: np.ndarray,        # mean relative position vector r = r1 - r2 (shape (3,))\n",
    "    cov_rel_km2: np.ndarray,       # combined covariance matrix (3x3) in km^2\n",
    "    hbr_km: float,                 # combined hard-body radius in km\n",
    "    n_samples: int = 200_000,\n",
    "    rng_seed: Optional[int] = 0\n",
    ") -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Monte Carlo estimate of collision probability.\n",
    "    Returns (p_est, stderr).\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(rng_seed)\n",
    "\n",
    "    # numerical jitter for near-singular covariance\n",
    "    jitter = 1e-12 * np.trace(cov_rel_km2) if np.isfinite(np.trace(cov_rel_km2)) else 1e-12\n",
    "    cov_use = cov_rel_km2 + np.eye(3) * jitter\n",
    "\n",
    "    # try cholesky, fallback to eigh\n",
    "    try:\n",
    "        L = la.cholesky(cov_use, lower=True)\n",
    "    except la.LinAlgError:\n",
    "        evals, evecs = la.eigh(cov_use)\n",
    "        evals_clipped = np.clip(evals, 0, None)\n",
    "        L = (evecs * np.sqrt(evals_clipped)) @ evecs.T\n",
    "\n",
    "    # draw standard normals, transform\n",
    "    z = rng.standard_normal(size=(n_samples, 3))\n",
    "    samples = rel_pos_km.reshape(1, 3) + (z @ L.T)  # shape (N,3)\n",
    "    d = np.linalg.norm(samples, axis=1)\n",
    "    hits = (d <= hbr_km)\n",
    "    p = hits.mean()\n",
    "    stderr = np.sqrt(p * (1 - p) / n_samples) if n_samples > 0 else 0.0\n",
    "    return float(p), float(stderr)\n",
    "\n",
    "# ----------------------------\n",
    "# Defaults (tweak as needed)\n",
    "# ----------------------------\n",
    "# Default positional sigma per object (ECI axes) in km [sigma_x, sigma_y, sigma_z]\n",
    "# We treat these as approximations in ECI. If you have RSW covs, transform them externally.\n",
    "DEFAULT_SIGMA_OBJ_KM = np.array([0.05, 0.50, 0.05])  # radial, along-track, cross-track approximations\n",
    "DEFAULT_RADIUS_M = 1.0  # default geometric radius in meters for each object\n",
    "DEFAULT_SAMPLES = 200_000\n",
    "\n",
    "# ----------------------------\n",
    "# Utility to build combined covariance matrix (diagonal) if no covariance provided\n",
    "# ----------------------------\n",
    "def build_combined_covariance(sigma_obj1_km, sigma_obj2_km):\n",
    "    \"\"\"\n",
    "    sigma_obj* : array-like length 3 (sigma_x, sigma_y, sigma_z) in km\n",
    "    returns combined covariance C = diag(sigma1^2 + sigma2^2)\n",
    "    \"\"\"\n",
    "    s1 = np.asarray(sigma_obj1_km).reshape(3)\n",
    "    s2 = np.asarray(sigma_obj2_km).reshape(3)\n",
    "    C = np.diag(s1**2 + s2**2)\n",
    "    return C\n",
    "\n",
    "# ----------------------------\n",
    "# Main batch function\n",
    "# ----------------------------\n",
    "def compute_collision_probabilities(\n",
    "    conjunctions_df: pd.DataFrame,\n",
    "    get_relpos_func=None,\n",
    "    # get_relpos_func(row) -> np.array([rx,ry,rz]) relative vector in km at TCA\n",
    "    # if None, will try to use column 'r_rel_km' in df\n",
    "    sigma_defaults_km: np.ndarray = DEFAULT_SIGMA_OBJ_KM,\n",
    "    radius_default_m: float = DEFAULT_RADIUS_M,\n",
    "    n_samples: int = DEFAULT_SAMPLES,\n",
    "    random_seed: int = 0\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    For each row in conjunctions_df, estimate Pc. Returns an expanded DataFrame with columns:\n",
    "      rel_x, rel_y, rel_z, cov_trace, HBR_km, Pc, Pc_stderr.\n",
    "    Required columns (if get_relpos_func is None): 'r_rel_km' (iterable of 3), 'Object_A', 'Object_B', 'TCA'\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    for idx, row in conjunctions_df.iterrows():\n",
    "        # obtain relative vector\n",
    "        if get_relpos_func is not None:\n",
    "            rel = get_relpos_func(row)\n",
    "        else:\n",
    "            if \"r_rel_km\" in row and pd.notna(row[\"r_rel_km\"]):\n",
    "                # assume stored as list/tuple or string like \"[x, y, z]\"\n",
    "                val = row[\"r_rel_km\"]\n",
    "                if isinstance(val, str):\n",
    "                    import ast\n",
    "                    rel = np.asarray(ast.literal_eval(val), dtype=float)\n",
    "                else:\n",
    "                    rel = np.asarray(val, dtype=float)\n",
    "            else:\n",
    "                # fallback: use miss_distance direction-agnostic â€” place along x axis\n",
    "                if \"Miss_Distance_km\" in row and not pd.isna(row[\"Miss_Distance_km\"]):\n",
    "                    d = float(row[\"Miss_Distance_km\"])\n",
    "                else:\n",
    "                    d = float(row.get(\"miss_distance\", np.nan) or np.nan)\n",
    "                if np.isnan(d):\n",
    "                    # cannot proceed: skip\n",
    "                    rows.append({**row.to_dict(), \"Pc\": np.nan, \"Pc_stderr\": np.nan, \"note\": \"no_relpos_no_missdist\"})\n",
    "                    continue\n",
    "                rel = np.array([d, 0.0, 0.0])\n",
    "\n",
    "        # build covariance (diagonal) for both objects (use defaults or per-row if provided)\n",
    "        sigma_a = row.get(\"sigma_objA_km\", None) or sigma_defaults_km\n",
    "        sigma_b = row.get(\"sigma_objB_km\", None) or sigma_defaults_km\n",
    "        sigma_a = np.asarray(sigma_a, dtype=float).reshape(3)\n",
    "        sigma_b = np.asarray(sigma_b, dtype=float).reshape(3)\n",
    "        C = build_combined_covariance(sigma_a, sigma_b)\n",
    "\n",
    "        # HBR: per-row radii override or default\n",
    "        rA_m = row.get(\"radius_A_m\", None) or radius_default_m\n",
    "        rB_m = row.get(\"radius_B_m\", None) or radius_default_m\n",
    "        hbr_km = (float(rA_m) + float(rB_m)) / 1000.0\n",
    "\n",
    "        # Estimate Pc\n",
    "        try:\n",
    "            p_est, se = estimate_pc_monte_carlo(rel, C, hbr_km, n_samples=n_samples, rng_seed=random_seed)\n",
    "        except Exception as e:\n",
    "            p_est, se = np.nan, np.nan\n",
    "\n",
    "        res = dict(row)\n",
    "        res.update({\n",
    "            \"rel_x_km\": float(rel[0]), \"rel_y_km\": float(rel[1]), \"rel_z_km\": float(rel[2]),\n",
    "            \"cov_trace_km2\": float(np.trace(C)),\n",
    "            \"HBR_km\": hbr_km,\n",
    "            \"Pc\": p_est,\n",
    "            \"Pc_stderr\": se\n",
    "        })\n",
    "        rows.append(res)\n",
    "\n",
    "    out_df = pd.DataFrame(rows)\n",
    "    return out_df\n",
    "\n",
    "# ----------------------------\n",
    "# Example usage:\n",
    "# ----------------------------\n",
    "# Suppose you have 'risky_df' with columns ['Object_A','Object_B','Miss_Distance_km','TCA'].\n",
    "# If you also computed the relative ECI vector at TCA for each pair, put it in column 'r_rel_km' (list or string).\n",
    "#\n",
    "# results_df = compute_collision_probabilities(risky_df, get_relpos_func=None,\n",
    "#                                              sigma_defaults_km=np.array([0.05,0.5,0.05]),\n",
    "#                                              radius_default_m=1.0,\n",
    "#                                              n_samples=200000, random_seed=42)\n",
    "#\n",
    "# results_df.to_csv(\"collision_probabilities_estimates.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07bfe3cf-1e37-4818-a7ee-239256ad1458",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "results_df = compute_collision_probabilities(joined_df, get_relpos_func=None,\n",
    "                                             sigma_defaults_km=np.array([0.05,0.5,0.05]),\n",
    "                                             radius_default_m=1.0,\n",
    "                                             n_samples=200000, random_seed=42)\n",
    "                                             \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f8f4716-4b4c-4366-b725-49044a070682",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2315652-5d68-4ba3-9f52-f9f2eb46a0a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install skyfield"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2803a7f2-8f2b-413b-a651-fcc1a987a8db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#another approach\n",
    "from skyfield.api import EarthSatellite, load\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "ts = load.timescale()\n",
    "\n",
    "# Get x,y,z for a TLE at a given date\n",
    "def get_xyz(tle1, tle2, target_date):\n",
    "    sat = EarthSatellite(tle1, tle2)\n",
    "    year, month, day = map(int, target_date.split(\"-\"))\n",
    "    t = ts.utc(year, month, day)\n",
    "    geocentric = sat.at(t)\n",
    "    pos = geocentric.position.km\n",
    "    return pos[0], pos[1], pos[2]\n",
    "\n",
    "# Distance function\n",
    "def distance_km(p1, p2):\n",
    "    return np.linalg.norm(np.array(p1) - np.array(p2))\n",
    "\n",
    "# Simplified collision probability\n",
    "def collision_probability(d, sigma=0.1, area=0.0013):\n",
    "    if d == np.inf:\n",
    "        return 0.0\n",
    "    return (area / (2 * np.pi * sigma**2)) * np.exp(-d**2 / (2 * sigma**2))\n",
    "\n",
    "# Main function\n",
    "def compute_conjunctions(df, target_date=\"2025-09-10\", top_n=10, risk_threshold_km=10):\n",
    "    results = []\n",
    "    positions = {}\n",
    "\n",
    "    # Precompute positions\n",
    "    for idx, row in df.iterrows():\n",
    "        positions[row.OBJECT_ID] = get_xyz(row.TLE_LINE1, row.TLE_LINE2, target_date)\n",
    "\n",
    "    object_ids = list(positions.keys())\n",
    "\n",
    "    # Pairwise distances\n",
    "    for i in range(len(object_ids)):\n",
    "        for j in range(i+1, len(object_ids)):\n",
    "            obj1, obj2 = object_ids[i], object_ids[j]\n",
    "            pos1, pos2 = positions[obj1], positions[obj2]\n",
    "            d = distance_km(pos1, pos2)\n",
    "            Pc = collision_probability(d)\n",
    "            results.append({\n",
    "                \"Object_1\": obj1,\n",
    "                \"Object_2\": obj2,\n",
    "                \"x1\": pos1[0], \"y1\": pos1[1], \"z1\": pos1[2],\n",
    "                \"x2\": pos2[0], \"y2\": pos2[1], \"z2\": pos2[2],\n",
    "                \"Miss_Distance_km\": d,\n",
    "                \"Collision_Probability\": Pc,\n",
    "                \"Risk_Flag\": d < risk_threshold_km\n",
    "            })\n",
    "\n",
    "    result_df = pd.DataFrame(results).sort_values(\"Miss_Distance_km\").head(top_n)\n",
    "    display(result_df)\n",
    "    return result_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34c8b2f6-9102-409a-bc35-66336ce05374",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "result_df = compute_conjunctions(df, target_date=\"2025-09-10\", top_n=10)\n",
    "display(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9887d898-480b-438d-918f-d6418966f77f",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1756955994852}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(result_df[result_df[\"Risk_Flag\"] == True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0eae15d-45c2-4a26-a359-5e8a837dce49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read the data from the Delta table\n",
    "leo_df = spark.read.table(\"workspace.default.objects_in_LEO\").toPandas()\n",
    "\n",
    "# Join result_df with leo_df to get additional information for Object_1\n",
    "result_df = result_df.merge(leo_df[[\"OBJECT_ID\", \"OBJECT_TYPE\"]], \n",
    "                            left_on=\"Object_1\", \n",
    "                            right_on=\"OBJECT_ID\", \n",
    "                            how=\"left\").rename(columns={\"OBJECT_TYPE\": \"Object_Type_1\"}).drop(columns=[\"OBJECT_ID\"])\n",
    "\n",
    "# Join again to get additional information for Object_2\n",
    "result_df = result_df.merge(leo_df[[\"OBJECT_ID\", \"OBJECT_TYPE\"]], \n",
    "                            left_on=\"Object_2\", \n",
    "                            right_on=\"OBJECT_ID\", \n",
    "                            how=\"left\").rename(columns={\"OBJECT_TYPE\": \"Object_Type_2\"}).drop(columns=[\"OBJECT_ID\"])\n",
    "\n",
    "# Display the updated DataFrame\n",
    "display(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "656f6bff-d1d8-4d7e-a0fa-13404cccfaaa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Propagate_densely_populated_objects_0904",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
