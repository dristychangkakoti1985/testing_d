{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e231ec5d-4b3a-43a8-a8db-86f09f08cdf8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Path to your extracted file\n",
    "file_path = \"/Volumes/workspace/default/miningaccidents/Accidents.txt\"  # Replace with your local or cloud path\n",
    "\n",
    "# Load the file using pipe delimiter and quoted values\n",
    "df = pd.read_csv(\n",
    "    file_path,\n",
    "    sep=\"|\",\n",
    "    quotechar='\"',\n",
    "    encoding=\"latin1\",  # Use 'latin1' encoding to handle decoding errors\n",
    "    dtype=str  # Start with all fields as strings\n",
    ")\n",
    "\n",
    "# Strip whitespace from column names and convert to lowercase\n",
    "df.columns = [col.strip().lower().replace(\" \", \"_\") for col in df.columns]\n",
    "\n",
    "# Convert relevant date fields\n",
    "df['accident_dt'] = pd.to_datetime(df['accident_dt'], errors='coerce')\n",
    "df['return_to_work_dt'] = pd.to_datetime(df['return_to_work_dt'], errors='coerce')\n",
    "\n",
    "# Drop rows missing crucial information (e.g., mine_id or accident_dt)\n",
    "df = df.dropna(subset=[\"mine_id\", \"accident_dt\"])\n",
    "\n",
    "# Preview cleaned data\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc758768-399a-4a0a-834f-4073ab0afb5b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "# Analyze the top 10 locations from ug_location column\n",
    "top_locations = df['ug_location_cd'].value_counts().head(10)\n",
    "\n",
    "display(top_locations)\n",
    "\n",
    "# fips_state_cd:object\n",
    "# ug_location_cd:object\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34699538-c8ea-48e8-8e88-9011e7f693e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Analyze the most common types of accidents\n",
    "common_accidents = df['accident_type'].value_counts().head(10)\n",
    "\n",
    "display(common_accidents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "837ca8fd-82c5-4cf5-b314-e86f36ccdf41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Gettng address data\n",
    "#/Volumes/workspace/default/miningaccidents/AddressOfRecord.txt\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Path to your extracted file\n",
    "file_path_add = \"/Volumes/workspace/default/miningaccidents/AddressOfRecord.txt\"  # Replace with your local or cloud path\n",
    "\n",
    "# Load the file using pipe delimiter and quoted values\n",
    "df_add = pd.read_csv(\n",
    "    file_path_add,\n",
    "    sep=\"|\",\n",
    "    quotechar='\"',\n",
    "    encoding=\"latin1\",  # Use 'latin1' encoding to handle decoding errors\n",
    "    dtype=str  # Start with all fields as strings\n",
    ")\n",
    "\n",
    "# Strip whitespace from column names and convert to lowercase\n",
    "df_add.columns = [col.strip().lower().replace(\" \", \"_\") for col in df_add.columns]\n",
    "\n",
    "# print(df_add.head())\n",
    "df_add.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8c1ef5f-10b7-457c-928f-d67e2c04cfce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "%pip install tabulate\n",
    "import tabulate\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Path to the downloaded file\n",
    "file_path = '/Volumes/workspace/default/miningaccidents/lite.json'  # Update this if needed\n",
    "\n",
    "# Step 1: Open and load JSON from the file\n",
    "with open(file_path, 'r', encoding='utf-8') as f:\n",
    "    stations = json.load(f)\n",
    "\n",
    "# # Load the data again in case the previous state is not preserved\n",
    "df = pd.read_json(file_path)\n",
    "\n",
    "# Flatten the 'name' column\n",
    "df_name = pd.json_normalize(df['name'], sep='_')\n",
    "df = pd.concat([df, df_name], axis=1)\n",
    "\n",
    "# Flatten the 'identifiers' column\n",
    "df_identifiers = pd.json_normalize(df['identifiers'], sep='_')\n",
    "df = pd.concat([df, df_identifiers], axis=1)\n",
    "\n",
    "# Flatten the 'location' column\n",
    "df_location = pd.json_normalize(df['location'], sep='_')\n",
    "df = pd.concat([df, df_location], axis=1)\n",
    "\n",
    "# Flatten the 'inventory' column\n",
    "df_inventory = pd.json_normalize(df['inventory'], sep='_')\n",
    "df = pd.concat([df, df_inventory], axis=1)\n",
    "\n",
    "# Drop the original nested columns\n",
    "df = df.drop(columns=['name', 'identifiers', 'location', 'inventory'])\n",
    "\n",
    "# Display the first 5 rows of the flattened DataFrame\n",
    "print(df.head().to_markdown(index=False, numalign=\"left\", stralign=\"left\"))\n",
    "\n",
    "# Print the column names and their data types\n",
    "#print(df.info())\n",
    "df.display()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Prep Mining data",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
